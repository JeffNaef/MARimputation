# impnorm<-X
# impnorm[is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var))
#
# ##Calculate theta for each imputation
# model<-lm(X2~X1, X=X.frame(implist[[j]]))
# hattheta<-model$coefficient["X1"]
# vartheta <- vcov(model)[2,2]
#
# # CI:
# CIupper <- hattheta + qnorm(0.975)*sqrt(vartheta)
# CIlower <- hattheta - qnorm(0.975)*sqrt(vartheta)
#
# print( paste("lower: ", round(CIlower,2), "upper: ", round(CIupper,2)  ) )
##Create m imputations
implist<-list()
hattheta_norm<-c()
varvec<-c()
for (j in 1:m){
implist[[j]]<-X
implist[[j]][is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var))
##Calculate theta for each imputation
modelj<-lm(X2~X1, data=data.frame(implist[[j]]))
hattheta_norm[j]<-modelj$coefficient["X1"]
varvec[j] <- vcov(modelj)[2,2]
}
## Mean Estimate:
hattheta <- mean(hattheta_norm)
# First part of variance
Ubar<-mean(varvec)
# Second part of variance
B <- var(hattheta_norm)
Tvar <- Ubar + (1+1/m)*B
# CI:
CIupper <- hattheta + qnorm(0.975)*sqrt(Tvar)
CIlower <- hattheta - qnorm(0.975)*sqrt(Tvar)
print( paste("lower: ", round(CIlower,2), "upper: ", round(CIupper,2)  ) )
library(mice)
## Naniar provides principled, tidy ways to summarise, visualise, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data.
library(naniar)
library(VIM)
library(FactoMineR)
X<- read.csv("data.csv", header=T, row.names=1)
Xstar<- read.csv("fulldata.csv", header=T, row.names=1)
head(X)
head(Xstar)
summary(X)
# also on Rmistatic maybe you can use the same simulations: https://rmisstastic.netlify.app/how-to/impute/missimp
res<-summary(aggr(X, sortVar=TRUE))$combinations
res[rev(order(res[,2])),]
#The VIM function matrixplot creates a matrix plot in which all cells of a X matrix are visualized by rectangles. Available X is coded according to a continuous color scheme (gray scale), while missing/imputed X is visualized by a clearly distinguishable color (red). If you use Rstudio the plot is not interactive (there are the warnings), but if you use R directly, you can click on a column of your choice: the rows are sorted (decreasing order) of the values of this column. This is useful to check if there is an association between the value of a variable and the missingness of another one.
matrixplot(X, sortby = 3)
marginplot(X[,2:3])
#| output: false
library(mice)
source("Iscore.R")
X<-as.matrix(X)
methods<-c("pmm",      # mice-pmm
"cart",     # mice-cart
"rf",       #mice-rf
"sample",   #mice-sample
"norm.nob") # Gaussian Imputation
imputationfuncs<-list()
for (method in methods){
##Probably not the smartes solution
imputationfuncs[[method]][[1]] <- method
imputationfuncs[[method]][[2]] <- function(X,m, method, maxit=5){
tmp<-mice(X, m = m, method = method,  printFlag = F, visitSequence="arabic", maxit = maxit)
return(mice::complete(tmp, action="all"))
}
}
scoreslist <- Iscores_new(X,imputations=NULL,score="mIScore2", imputationfuncs=imputationfuncs, N=30)
scores<-do.call(cbind,lapply(scoreslist, function(x) x$score ))
names(scores)<-methods
scores[order(scores)]
imp.mice <- mice(X, m = 10, method = "cart", printFlag = F)
Ximp<-mice::complete(imp.mice)
index1<-3
index2<-6
par(mfrow=c(1,2))
plot(Xstar[is.na(X[,index1]) | is.na(X[,index2]),c(index1,index2)])
plot(Ximp[is.na(X[,index1]) | is.na(X[,index2]),c(index1,index2)])
# However: Despite the fact that mice-cart appears very good, there are problems:
hist(Xstar[,6])
Ximp<-mice::complete(imp.mice)
index1<-3
index2<-6
par(mfrow=c(1,2))
plot(Xstar[is.na(X[,index1]) | is.na(X[,index2]),c(index1,index2)])
plot(Ximp[is.na(X[,index1]) | is.na(X[,index2]),c(index1,index2)])
colMeans(Xstar)-colMeans(Ximp)
norm(cov(Xstar) - cov(Ximp))
# However: Despite the fact that mice-cart appears very good, there are problems:
hist(Xstar[,6])
hist(Ximp[,6], add=T)
# However: Despite the fact that mice-cart appears very good, there are problems:
hist(Xstar[,6])
hist(Ximp[,6], add=T)
hist(Xstar[,6])
hist(Ximp[,6], add=T, col="green")
hist(Xstar[,6])
plot(Xstar[,3:6])
plot(Xstar[,3:6])
points(Ximp[,3:6], col="darkblue")
plot(Xstar[,c(3,6)])
points(Ximp[,c(3,6)], col="darkblue")
plot(Xstar[,c(3,6)])
points(Ximp[,c(3,6)], col="darkblue")
## This here is not possible without the fully observed data ###
Ximp<-mice::complete(imp.mice)
index2<-2
par(mfrow=c(1,2))
plot(Ximp[is.na(X[,index1]) | is.na(X[,index2]),c(index1,index2)])
colMeans(Xstar)-colMeans(Ximp)
norm(cov(Xstar) - cov(Ximp))
cov(Xstar)
cov(Xstar[,c(3,6)])
cov(Ximp[,c(3,6)])
cor(Ximp[,c(3,6)])
cor(Xstar[,c(3,6)])
plot(Xstar[,c(3,6)])
plot(Xstar[,c(3,6)])
points(Ximp[,c(3,6)], col="darkblue")
plot(Xstar[,c(3,6)])
norm(cov(Xstar) - cov(Ximp))/norm(cov(Xstar))
summary(res)
# Apply a regression to the mulitple imputation
lm.mice.out <- with(imp.mice, lm(max_O3 ~ max_PM2.5 +Longitude +Latitude +Elevation +Land.Use_AGRICULTURAL+Land.Use_COMMERCIAL+Land.Use_INDUSTRIAL+Location.Setting_RURAL+Location.Setting_SUBURBAN))
# Use Rubins Rules to aggregate the estimates
res<-pool(lm.mice.out)
summary(res)
## This here is not possible without the fully observed data ###
res.not.attainable<-lm(max_O3 ~ max_PM2.5 +Longitude +Latitude +Elevation +Land.Use_AGRICULTURAL+Land.Use_COMMERCIAL+Land.Use_INDUSTRIAL+Location.Setting_RURAL+Location.Setting_SUBURBAN, data=as.data.frame(Xstar))
summary(res.not.attainable)
res$pooled
res$pooled$estimate
cbind(res$pooled$estimate, res.not.attainable)
cbind(res$pooled$estimate, res.not.attainable$coefficients)
(res$pooled$estimate-res.not.attainable$coefficients)/res.not.attainable$coefficients
t((res$pooled$estimate-res.not.attainable$coefficients)/res.not.attainable$coefficients)
cbind((res$pooled$estimate-res.not.attainable$coefficients)/res.not.attainable$coefficients)
cbind(round( (res$pooled$estimate-res.not.attainable$coefficients)/res.not.attainable$coefficients, 3)  )
res.not.attainable$coefficients
res$pooled$estimate
#| warning: false
###Code adapted from Stef van Buuren's book
###https://stefvanbuuren.name/fimd/sec-true.html
library(mice)
library(energy)
create.X <- function(beta = 1, sigma2 = 2, n = 50,
run = 1) {
set.seed(seed = run)
x <- rnorm(n)
y <- beta * x + rnorm(n, sd = sqrt(sigma2))
cbind(X1 = x, X2 = y)
}
set.seed(10)
n<-2000
Xstar <- create.X(run = 1, n=n)
plot(Xstar, cex=0.8, col="darkblue")
## MCAR Example
#M<-cbind(sample(c(0,1), size=nrow(Xstar), replace=T, prob = c(1-0.2,0.2) ), 0  )
#X<-Xstar
#X[M==1] <- NA
## MAR Example
M<-matrix(0, nrow=nrow(Xstar), ncol=ncol(Xstar))
M[Xstar[,2] > 0, 1] <- sample(c(0,1), size=sum(Xstar[,2] > 0), replace=T, prob = c(1-0.8,0.8) )
colnames(M) <- cbind("M1", "M2")
#M[Xstar[,2] > 0, 1] <- sample(c(0,1), size=sum(Xstar[,2] > 0), replace=T, prob = c(0,1) )
X<-Xstar
X[M==1] <- NA
head(cbind(X, M))
head(Xstar)
## (0) Mean Imputation ##
# 1. Estimate the mean
meanX1<-mean(X[!is.na(X[,1]),1])
## 2. Impute
meanimp<-X
meanimp[is.na(X[,1]),1] <-meanX1
## (1) Regression Imputation ##
# 1. Estimate Regression
lmodelX1X2<-lm(X1~X2, data=as.data.frame(X[!is.na(X[,1]),])   )
## 2. Impute
impnormpredict<-X
impnormpredict[is.na(X[,1]),1] <-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )
## (2) Gaussian Imputation ##
# 1. Estimate Regression
#lmodelX1X2<-lm(X1~X2, X=as.X.frame(X[!is.na(X[,1]),])   )
# (same as before)
## 2. Impute
impnorm<-X
meanx<-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )
var <- var(lmodelX1X2$residuals)
impnorm[is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var) )
##RMSE calculation
RMSEcalc<-function(impX){
round(mean(apply(Xstar - impX,1,function(x) norm(as.matrix(x), type="F"  ) )),2)
}
energycalc <- function(impX){
round(eqdist.e( rbind(Xstar,impX), c(nrow(Xstar), nrow(impX))  ),2)
}
# Plot the Imputations
par(mfrow=c(2,2))
plot(meanimp[!is.na(X[,1]),c("X2","X1")], main=paste("Mean Imputation", "\nRMSE", RMSEcalc(meanimp), "\nEnergy", energycalc(meanimp)), cex=0.8, col="darkblue", cex.main=1.5)
points(meanimp[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
plot(impnormpredict[!is.na(X[,1]),c("X2","X1")], main=paste("Regression Imputation","\nRMSE", RMSEcalc(impnormpredict), "\nEnergy", energycalc(impnormpredict)), cex=0.8, col="darkblue", cex.main=1.5)
points(impnormpredict[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
plot(impnorm[!is.na(X[,1]),c("X2","X1")], main=paste("Gaussian Imputation","\nRMSE", RMSEcalc(impnorm), "\nEnergy", energycalc(impnorm)), col="darkblue", cex.main=1.5)
points(impnorm[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
#plot(Xstar[,c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)
plot(Xstar[!is.na(X[,1]),c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)
points(Xstar[is.na(X[,1]),c("X2","X1")], col="darkgreen", cex=0.8 )
# Plot the Imputations
par(mfrow=c(2,2))
plot(meanimp[!is.na(X[,1]),c("X2","X1")], main=paste("Mean Imputation", "\nRMSE", RMSEcalc(meanimp), "\nEnergy", energycalc(meanimp)), cex=0.8, col="darkblue", cex.main=1.5)
points(meanimp[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
plot(impnormpredict[!is.na(X[,1]),c("X2","X1")], main=paste("Regression Imputation","\nRMSE", RMSEcalc(impnormpredict), "\nEnergy", energycalc(impnormpredict)), cex=0.8, col="darkblue", cex.main=1.5)
points(impnormpredict[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
plot(impnorm[!is.na(X[,1]),c("X2","X1")], main=paste("Gaussian Imputation","\nRMSE", RMSEcalc(impnorm), "\nEnergy", energycalc(impnorm)), col="darkblue", cex.main=1.5)
points(impnorm[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
#plot(Xstar[,c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)
plot(Xstar[!is.na(X[,1]),c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)
points(Xstar[is.na(X[,1]),c("X2","X1")], col="darkgreen", cex=0.8 )
#| warning: false
library(optimx)
X1<-X[!is.na(X[,1]),1]
X2m1<-X[!is.na(X[,1]),2]
X2m2<-X[is.na(X[,1]),2]
f <- function(theta){
Likm1<-sum(dnorm(X2m1, mean=theta*X1, sd=sqrt(2), log=T))
Likm2<-sum(dnorm(X2m2, mean=0, sd=sqrt(theta^2+2), log=T))
return( -(Likm1 + Likm2))
}
res <- optimx(0, fn = f, method = "BFGS", control=list(dowarn=F))
print( paste0("Correct Likelihood Optimization: ", round(res$p1,2) ) )
## Complete-case analysis is biased!
lmodelX2X1<-lm(X2~X1, data=as.data.frame(X[!is.na(X[,1]),])   )
hatthetaMLE <- lmodelX2X1$coefficients["X1"]
print( paste0("Complete-case analysis: ", round(hatthetaMLE,2) ) )
hattheta_meanimp<-lm(X2~X1, data=data.frame(meanimp))$coefficient["X1"]
hattheta_normpredict<-lm(X2~X1, data=data.frame(impnormpredict))$coefficient["X1"]
hattheta_norm<-lm(X2~X1, data=data.frame(impnorm))$coefficient["X1"]
print(paste0("Mean Imputation: ", round(hattheta_meanimp,2)) )
print(paste0("Mean Imputation: ", round(hattheta_normpredict,2)) )
print(paste0("Mean Imputation: ", round(hattheta_norm,2)) )
set.seed(10)
n<-2000
m<-50
m<-50
m<-50
##Create m imputations
implist<-list()
hattheta_norm<-c()
varvec<-c()
implist[[j]]<-X
j<-1
implist[[j]]<-X
implist[[j]][is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var))
##Calculate theta for each imputation
modelj<-lm(X2~X1, data=data.frame(implist[[j]]))
hattheta_norm[j]<-modelj$coefficient["X1"]
varvec[j] <- vcov(modelj)[2,2]
implist<-list()
hattheta_norm<-c()
varvec<-c()
for (j in 1:m){
implist[[j]]<-X
implist[[j]][is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var))
##Calculate theta for each imputation
modelj<-lm(X2~X1, data=data.frame(implist[[j]]))
hattheta_norm[j]<-modelj$coefficient["X1"]
varvec[j] <- vcov(modelj)[2,2]
}
## Mean Estimate:
hattheta <- mean(hattheta_norm)
# First part of variance
Ubar<-mean(varvec)
hattheta_norm
# Second part of variance
B <- var(hattheta_norm)
Tvar <- Ubar + (1+1/m)*B
Tvar
# CI:
CIupper <- hattheta + qnorm(0.975)*sqrt(Tvar)
CIlower <- hattheta - qnorm(0.975)*sqrt(Tvar)
print( paste("lower: ", round(CIlower,2), "upper: ", round(CIupper,2)  ) )
library(mice)
## Naniar provides principled, tidy ways to summarise, visualise, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data.
library(naniar)
library(VIM)
library(FactoMineR)
X<- read.csv("data.csv", header=T, row.names=1)
Xstar<- read.csv("fulldata.csv", header=T, row.names=1)
head(X)
head(Xstar)
summary(X)
res<-summary(aggr(X, sortVar=TRUE))$combinations
res[rev(order(res[,2])),]
res[rev(order(res[,2])),]
matrixplot(X, sortby = 3)
marginplot(X[,2:3])
#| output: false
library(mice)
source("Iscore.R")
X<-as.matrix(X)
methods<-c("pmm",      # mice-pmm
"cart",     # mice-cart
"rf",       #mice-rf
"sample",   #mice-sample
"norm.nob") # Gaussian Imputation
imputationfuncs<-list()
for (method in methods){
##Probably not the smartes solution
imputationfuncs[[method]][[1]] <- method
imputationfuncs[[method]][[2]] <- function(X,m, method, maxit=5){
tmp<-mice(X, m = m, method = method,  printFlag = F, visitSequence="arabic", maxit = maxit)
return(mice::complete(tmp, action="all"))
}
}
scoreslist <- Iscores_new(X,imputations=NULL,score="mIScore2", imputationfuncs=imputationfuncs, N=30)
scores<-do.call(cbind,lapply(scoreslist, function(x) x$score ))
names(scores)<-methods
scores[order(scores)]
imp.mice <- mice(X, m = 10, method = "cart", printFlag = F)
# Apply a regression to the mulitple imputation
lm.mice.out <- with(imp.mice, lm(max_O3 ~ max_PM2.5 +Longitude +Latitude +Elevation +Land.Use_AGRICULTURAL+Land.Use_COMMERCIAL+Land.Use_INDUSTRIAL+Location.Setting_RURAL+Location.Setting_SUBURBAN))
# Use Rubins Rules to aggregate the estimates
res<-pool(lm.mice.out)
summary(res)
## This here is not possible without the fully observed data ###
res.not.attainable<-lm(max_O3 ~ max_PM2.5 +Longitude +Latitude +Elevation +Land.Use_AGRICULTURAL+Land.Use_COMMERCIAL+Land.Use_INDUSTRIAL+Location.Setting_RURAL+Location.Setting_SUBURBAN, data=as.data.frame(Xstar))
summary(res.not.attainable)
cbind(round( (res$pooled$estimate-res.not.attainable$coefficients)/res.not.attainable$coefficients, 3)  )
# Plot the Imputations
par(mfrow=c(2,2))
plot(meanimp[!is.na(X[,1]),c("X2","X1")], main=paste("Mean Imputation", "\nRMSE", RMSEcalc(meanimp), "\nEnergy", energycalc(meanimp)), cex=0.8, col="darkblue", cex.main=1.5)
#| warning: false
###Code adapted from Stef van Buuren's book
###https://stefvanbuuren.name/fimd/sec-true.html
library(mice)
library(energy)
create.X <- function(beta = 1, sigma2 = 2, n = 50,
run = 1) {
set.seed(seed = run)
x <- rnorm(n)
y <- beta * x + rnorm(n, sd = sqrt(sigma2))
cbind(X1 = x, X2 = y)
}
set.seed(10)
n<-2000
Xstar <- create.X(run = 1, n=n)
plot(Xstar, cex=0.8, col="darkblue")
## MCAR Example
#M<-cbind(sample(c(0,1), size=nrow(Xstar), replace=T, prob = c(1-0.2,0.2) ), 0  )
#X<-Xstar
#X[M==1] <- NA
## MAR Example
M<-matrix(0, nrow=nrow(Xstar), ncol=ncol(Xstar))
M[Xstar[,2] > 0, 1] <- sample(c(0,1), size=sum(Xstar[,2] > 0), replace=T, prob = c(1-0.8,0.8) )
colnames(M) <- cbind("M1", "M2")
#M[Xstar[,2] > 0, 1] <- sample(c(0,1), size=sum(Xstar[,2] > 0), replace=T, prob = c(0,1) )
X<-Xstar
X[M==1] <- NA
head(cbind(X, M))
head(Xstar)
## (0) Mean Imputation ##
# 1. Estimate the mean
meanX1<-mean(X[!is.na(X[,1]),1])
## 2. Impute
meanimp<-X
meanimp[is.na(X[,1]),1] <-meanX1
## (1) Regression Imputation ##
# 1. Estimate Regression
lmodelX1X2<-lm(X1~X2, data=as.data.frame(X[!is.na(X[,1]),])   )
## 2. Impute
impnormpredict<-X
impnormpredict[is.na(X[,1]),1] <-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )
## (2) Gaussian Imputation ##
# 1. Estimate Regression
#lmodelX1X2<-lm(X1~X2, X=as.X.frame(X[!is.na(X[,1]),])   )
# (same as before)
## 2. Impute
impnorm<-X
meanx<-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )
var <- var(lmodelX1X2$residuals)
impnorm[is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var) )
##RMSE calculation
RMSEcalc<-function(impX){
round(mean(apply(Xstar - impX,1,function(x) norm(as.matrix(x), type="F"  ) )),2)
}
energycalc <- function(impX){
round(eqdist.e( rbind(Xstar,impX), c(nrow(Xstar), nrow(impX))  ),2)
}
# Plot the Imputations
par(mfrow=c(2,2))
plot(meanimp[!is.na(X[,1]),c("X2","X1")], main=paste("Mean Imputation", "\nRMSE", RMSEcalc(meanimp), "\nEnergy", energycalc(meanimp)), cex=0.8, col="darkblue", cex.main=1.5)
points(meanimp[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
plot(impnormpredict[!is.na(X[,1]),c("X2","X1")], main=paste("Regression Imputation","\nRMSE", RMSEcalc(impnormpredict), "\nEnergy", energycalc(impnormpredict)), cex=0.8, col="darkblue", cex.main=1.5)
points(impnormpredict[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
plot(impnorm[!is.na(X[,1]),c("X2","X1")], main=paste("Gaussian Imputation","\nRMSE", RMSEcalc(impnorm), "\nEnergy", energycalc(impnorm)), col="darkblue", cex.main=1.5)
points(impnorm[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )
#plot(Xstar[,c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)
plot(Xstar[!is.na(X[,1]),c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)
points(Xstar[is.na(X[,1]),c("X2","X1")], col="darkgreen", cex=0.8 )
#| warning: false
library(optimx)
X1<-X[!is.na(X[,1]),1]
X2m1<-X[!is.na(X[,1]),2]
X2m2<-X[is.na(X[,1]),2]
f <- function(theta){
Likm1<-sum(dnorm(X2m1, mean=theta*X1, sd=sqrt(2), log=T))
Likm2<-sum(dnorm(X2m2, mean=0, sd=sqrt(theta^2+2), log=T))
return( -(Likm1 + Likm2))
}
res <- optimx(0, fn = f, method = "BFGS", control=list(dowarn=F))
print( paste0("Correct Likelihood Optimization: ", round(res$p1,2) ) )
## Complete-case analysis is biased!
lmodelX2X1<-lm(X2~X1, data=as.data.frame(X[!is.na(X[,1]),])   )
hatthetaMLE <- lmodelX2X1$coefficients["X1"]
print( paste0("Complete-case analysis: ", round(hatthetaMLE,2) ) )
hattheta_meanimp<-lm(X2~X1, data=data.frame(meanimp))$coefficient["X1"]
hattheta_normpredict<-lm(X2~X1, data=data.frame(impnormpredict))$coefficient["X1"]
hattheta_norm<-lm(X2~X1, data=data.frame(impnorm))$coefficient["X1"]
print(paste0("Mean Imputation: ", round(hattheta_meanimp,2)) )
print(paste0("Mean Imputation: ", round(hattheta_normpredict,2)) )
print(paste0("Mean Imputation: ", round(hattheta_norm,2)) )
hattheta_meanimp<-lm(X2~X1, data=data.frame(meanimp))$coefficient["X1"]
hattheta_normpredict<-lm(X2~X1, data=data.frame(impnormpredict))$coefficient["X1"]
hattheta_norm<-lm(X2~X1, data=data.frame(impnorm))$coefficient["X1"]
print(paste0("Mean Imputation: ", round(hattheta_meanimp,2)) )
print(paste0("Regression Imputation: ", round(hattheta_normpredict,2)) )
print(paste0("Gaussian Imputation: ", round(hattheta_norm,2)) )
for (j in 1:m){
implist[[j]]<-X
implist[[j]][is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var))
##Calculate theta for each imputation
modelj<-lm(X2~X1, data=data.frame(implist[[j]]))
hattheta_norm[j]<-modelj$coefficient["X1"]
varvec[j] <- vcov(modelj)[2,2]
}
m<-20
##Create m imputations
implist<-list()
hattheta_norm<-c()
varvec<-c()
for (j in 1:m){
implist[[j]]<-X
implist[[j]][is.na(X[,1]),1] <-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var))
##Calculate theta for each imputation
modelj<-lm(X2~X1, data=data.frame(implist[[j]]))
hattheta_norm[j]<-modelj$coefficient["X1"]
varvec[j] <- vcov(modelj)[2,2]
}
## Mean Estimate:
hattheta <- mean(hattheta_norm)
# First part of variance
Ubar<-mean(varvec)
# Second part of variance
B <- var(hattheta_norm)
Tvar <- Ubar + (1+1/m)*B
# CI:
CIupper <- hattheta + qnorm(0.975)*sqrt(Tvar)
CIlower <- hattheta - qnorm(0.975)*sqrt(Tvar)
print( paste("lower: ", round(CIlower,2), "upper: ", round(CIupper,2)  ) )
library(mice)
## Naniar provides principled, tidy ways to summarise, visualise, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data.
library(naniar)
library(VIM)
library(FactoMineR)
X<- read.csv("data.csv", header=T, row.names=1)
Xstar<- read.csv("fulldata.csv", header=T, row.names=1)
head(X)
head(Xstar)
summary(X)
res<-summary(aggr(X, sortVar=TRUE))$combinations
res[rev(order(res[,2])),]
res[rev(order(res[,2])),]
# also on Rmistatic maybe you can use the same simulations: https://rmisstastic.netlify.app/how-to/impute/missimp
res<-summary(aggr(X, sortVar=TRUE))$combinations
res[rev(order(res[,2])),]
#The VIM function matrixplot creates a matrix plot in which all cells of a X matrix are visualized by rectangles. Available X is coded according to a continuous color scheme (gray scale), while missing/imputed X is visualized by a clearly distinguishable color (red). If you use Rstudio the plot is not interactive (there are the warnings), but if you use R directly, you can click on a column of your choice: the rows are sorted (decreasing order) of the values of this column. This is useful to check if there is an association between the value of a variable and the missingness of another one.
res<-summary(aggr(X, sortVar=TRUE))$combinations
matrixplot(X, sortby = 3)
matrixplot(X, sortby = 3)
marginplot(X[,2:3])
#| output: false
library(mice)
source("Iscore.R")
X<-as.matrix(X)
methods<-c("pmm",      # mice-pmm
"cart",     # mice-cart
"rf",       #mice-rf
"sample",   #mice-sample
"norm.nob") # Gaussian Imputation
imputationfuncs<-list()
for (method in methods){
##Probably not the smartes solution
imputationfuncs[[method]][[1]] <- method
imputationfuncs[[method]][[2]] <- function(X,m, method, maxit=5){
tmp<-mice(X, m = m, method = method,  printFlag = F, visitSequence="arabic", maxit = maxit)
return(mice::complete(tmp, action="all"))
}
}
scoreslist <- Iscores_new(X,imputations=NULL,score="mIScore2", imputationfuncs=imputationfuncs, N=30)
scores<-do.call(cbind,lapply(scoreslist, function(x) x$score ))
names(scores)<-methods
scores[order(scores)]
imp.mice <- mice(X, m = 10, method = "cart", printFlag = F)
# Apply a regression to the mulitple imputation
lm.mice.out <- with(imp.mice, lm(max_O3 ~ max_PM2.5 +Longitude +Latitude +Elevation +Land.Use_AGRICULTURAL+Land.Use_COMMERCIAL+Land.Use_INDUSTRIAL+Location.Setting_RURAL+Location.Setting_SUBURBAN))
# Use Rubins Rules to aggregate the estimates
res<-pool(lm.mice.out)
summary(res)
